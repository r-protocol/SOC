# main.py
import sys
import datetime
from db_utils import initialize_database, get_existing_urls, store_analyzed_data, store_iocs, store_kql_queries
from fetcher import fetch_and_scrape_articles_sequential, fetch_single_article
from filtering import filter_articles_sequential
from analysis import analyze_articles_sequential
from report import generate_weekly_report, get_last_full_week_dates
from logging_utils import log_step, log_warn, log_info, log_success, log_error, BColors
from config import ENABLE_KQL_GENERATION, KQL_EXPORT_DIR, KQL_EXPORT_ENABLED
from kql_generator_llm import LLMKQLGenerator
from kql_generator import save_queries_to_file, IOCExtractor as RegexIOCExtractor

def generate_kql_for_articles(article_ids):
    """Generate KQL queries for analyzed articles using LLM"""
    from logging_utils import BColors
    
    log_step("KQL", "Generating KQL Threat Hunting Queries (LLM-Enhanced)")
    llm_generator = LLMKQLGenerator()
    
    total_iocs = 0
    total_queries = 0
    all_queries = []
    
    for article_id, article_data in article_ids:
        # Extract IOCs and generate queries using LLM
        iocs, queries = llm_generator.generate_all(article_data)
        ioc_count = sum(len(iocs.get(key, [])) for key in iocs)
        
        if ioc_count > 0:
            # Store IOCs
            stored_iocs = store_iocs(article_id, iocs)
            total_iocs += stored_iocs
        
        # Store and track queries (already generated by LLM)
        if queries:
            stored_queries = store_kql_queries(article_id, queries)
            total_queries += stored_queries
            all_queries.extend(queries)
            log_info(f"Generated {len(queries)} LLM queries for '{article_data['title']}'")
    
    # Export queries to files if enabled
    if KQL_EXPORT_ENABLED and all_queries:
        import os
        os.makedirs(KQL_EXPORT_DIR, exist_ok=True)
        save_queries_to_file(all_queries, KQL_EXPORT_DIR)
        log_success(f"Exported {len(all_queries)} queries to '{KQL_EXPORT_DIR}/' directory")
    
    log_success(f"KQL Generation complete: {total_iocs} IOCs, {total_queries} queries stored")
    return all_queries


def prompt_kql_generation():
    """Prompt user to generate KQL queries"""
    from logging_utils import BColors
    
    print(f"\n{BColors.HEADER}{'='*70}{BColors.ENDC}")
    print(f"{BColors.OKCYAN}📊 KQL Threat Hunting Query Generator{BColors.ENDC}")
    print(f"{BColors.HEADER}{'='*70}{BColors.ENDC}")
    print(f"\n{BColors.OKBLUE}Would you like to generate KQL threat hunting queries?{BColors.ENDC}")
    print(f"{BColors.WARNING}This will:{BColors.ENDC}")
    print(f"  • Extract IOCs (IPs, domains, hashes, CVEs, URLs) from analyzed articles")
    print(f"  • Generate KQL queries for Microsoft Defender/Sentinel")
    print(f"  • Store queries in database and export to .kql files")
    print(f"\n{BColors.OKCYAN}Generate KQL queries? (y/n): {BColors.ENDC}", end='')
    
    try:
        response = input().strip().lower()
        return response in ['y', 'yes']
    except (KeyboardInterrupt, EOFError):
        print(f"\n{BColors.WARNING}Skipping KQL generation.{BColors.ENDC}")
        return False


def process_single_article(url, use_kql=False):
    """Process a single article from URL - fetch, analyze, and optionally generate KQL"""
    print(f"\n{BColors.HEADER}{'='*70}{BColors.ENDC}")
    print(f"{BColors.HEADER}🔍 Single Article Processing Mode{BColors.ENDC}")
    print(f"{BColors.HEADER}{'='*70}{BColors.ENDC}\n")
    
    # Step 1: Fetch article
    log_step(1, "Fetching Article")
    article = fetch_single_article(url)
    
    if not article:
        log_error("Failed to fetch article. Exiting.")
        return
    
    # Step 2: Filter for relevance
    log_step(2, "Checking Cybersecurity Relevance")
    relevant_articles = filter_articles_sequential([article])
    
    if not relevant_articles:
        log_warn("Article is not relevant to cybersecurity. Exiting.")
        return
    
    log_success(f"Article is relevant: {article['title']}")
    
    # Step 3: Analyze with LLM
    log_step(3, "Analyzing Article with LLM")
    analyzed_articles = analyze_articles_sequential(relevant_articles)
    
    if not analyzed_articles:
        log_error("Failed to analyze article. Exiting.")
        return
    
    analyzed_article = analyzed_articles[0]
    
    # Display analysis results
    print(f"\n{BColors.OKGREEN}{'='*70}{BColors.ENDC}")
    print(f"{BColors.OKGREEN}📊 Analysis Results{BColors.ENDC}")
    print(f"{BColors.OKGREEN}{'='*70}{BColors.ENDC}")
    print(f"{BColors.OKCYAN}Title:{BColors.ENDC} {analyzed_article['title']}")
    print(f"{BColors.OKCYAN}Category:{BColors.ENDC} {analyzed_article.get('category', 'N/A')}")
    print(f"{BColors.OKCYAN}Threat Risk:{BColors.ENDC} {analyzed_article.get('threat_risk', 'N/A')}")
    print(f"{BColors.OKCYAN}Summary:{BColors.ENDC}")
    print(f"  {analyzed_article.get('summary', 'N/A')[:300]}...")
    print(f"{BColors.OKGREEN}{'='*70}{BColors.ENDC}\n")
    
    # Step 4: Generate KQL if requested
    if use_kql:
        log_step(4, "Generating KQL Queries (LLM + Regex)")
        
        # LLM-based extraction and query generation
        print(f"\n{BColors.OKCYAN}🤖 LLM-Based IOC Extraction & KQL Generation{BColors.ENDC}")
        print(f"{BColors.OKCYAN}{'='*70}{BColors.ENDC}")
        
        llm_generator = LLMKQLGenerator()
        llm_iocs, llm_queries = llm_generator.generate_all(analyzed_article)
        
        # Display LLM IOCs
        total_llm_iocs = sum(len(llm_iocs.get(key, [])) for key in llm_iocs)
        print(f"\n{BColors.OKGREEN}✅ LLM Extracted {total_llm_iocs} IOCs:{BColors.ENDC}")
        
        for ioc_type, ioc_list in llm_iocs.items():
            if ioc_list:
                print(f"\n  {BColors.OKCYAN}{ioc_type.upper()}:{BColors.ENDC} ({len(ioc_list)})")
                for ioc in ioc_list[:5]:  # Show first 5
                    if isinstance(ioc, dict):
                        context = ioc.get('context', 'unknown')
                        confidence = ioc.get('confidence', 'unknown')
                        value = ioc.get('value', str(ioc))
                        desc = ioc.get('description', '')[:50]
                        print(f"    • {value}")
                        print(f"      Context: {context} | Confidence: {confidence}")
                        if desc:
                            print(f"      Description: {desc}...")
                    else:
                        print(f"    • {ioc}")
                if len(ioc_list) > 5:
                    print(f"    ... and {len(ioc_list) - 5} more")
        
        # Display LLM Queries
        print(f"\n{BColors.OKGREEN}✅ LLM Generated {len(llm_queries)} KQL Queries:{BColors.ENDC}")
        for i, query in enumerate(llm_queries, 1):
            print(f"\n  {BColors.OKCYAN}Query {i}: {query.get('name', 'Unnamed')}{BColors.ENDC}")
            print(f"    Type: {query.get('type', 'N/A')}")
            print(f"    Platform: {query.get('platform', 'N/A')}")
            print(f"    Description: {query.get('description', 'N/A')}")
            print(f"    Tables: {', '.join(query.get('tables', []))}")
        
        # Regex-based extraction for comparison
        print(f"\n{BColors.OKCYAN}⚡ Regex-Based IOC Extraction (for comparison){BColors.ENDC}")
        print(f"{BColors.OKCYAN}{'='*70}{BColors.ENDC}")
        
        regex_extractor = RegexIOCExtractor()
        regex_iocs = regex_extractor.extract_all(analyzed_article.get('content', ''))
        
        total_regex_iocs = sum(len(regex_iocs.get(key, [])) for key in regex_iocs)
        print(f"\n{BColors.OKGREEN}✅ Regex Extracted {total_regex_iocs} IOCs:{BColors.ENDC}")
        
        for ioc_type, ioc_list in regex_iocs.items():
            if ioc_list:
                print(f"  {ioc_type}: {len(ioc_list)}")
                for ioc in ioc_list[:3]:  # Show first 3
                    value = ioc.get('value', str(ioc)) if isinstance(ioc, dict) else ioc
                    print(f"    • {value}")
                if len(ioc_list) > 3:
                    print(f"    ... and {len(ioc_list) - 3} more")
        
        # Export queries
        if llm_queries:
            import os
            os.makedirs(KQL_EXPORT_DIR, exist_ok=True)
            save_queries_to_file(llm_queries, KQL_EXPORT_DIR)
            log_success(f"Exported {len(llm_queries)} queries to '{KQL_EXPORT_DIR}/' directory")
        
        # Summary comparison
        print(f"\n{BColors.HEADER}{'='*70}{BColors.ENDC}")
        print(f"{BColors.HEADER}📊 LLM vs Regex Comparison{BColors.ENDC}")
        print(f"{BColors.HEADER}{'='*70}{BColors.ENDC}")
        print(f"{BColors.OKCYAN}LLM:{BColors.ENDC} {total_llm_iocs} IOCs with context | {len(llm_queries)} queries")
        print(f"{BColors.OKCYAN}Regex:{BColors.ENDC} {total_regex_iocs} IOCs (no context) | Template-based")
        print(f"{BColors.OKGREEN}{'='*70}{BColors.ENDC}\n")
    
    log_success("Single article processing complete!")


def main_pipeline():
    # Parse the -n parameter if provided
    article_limit = None
    auto_kql = False  # Check for --auto-kql flag
    
    if "-n" in sys.argv:
        try:
            n_index = sys.argv.index("-n")
            if n_index + 1 < len(sys.argv):
                article_limit = int(sys.argv[n_index + 1])
        except (ValueError, IndexError):
            log_warn("Invalid -n parameter. Processing all articles.")
    
    if "--auto-kql" in sys.argv or "--kql" in sys.argv:
        auto_kql = True

    initialize_database()
    existing_urls = get_existing_urls()
    start_date, end_date = get_last_full_week_dates()
    
    # Phase 1: Fetch and Scrape all new articles
    log_step(1, "Fetching and Scraping New Articles")
    new_articles = fetch_and_scrape_articles_sequential(existing_urls, start_date, end_date)
    
    # Apply article limit if specified
    if article_limit and len(new_articles) > article_limit:
        new_articles = new_articles[:article_limit]
        log_info(f"Limited to {article_limit} articles as requested.")
    
    # Phase 2: Filter for relevant articles
    log_step(2, "Filtering New Articles for Cybersecurity Relevance")
    relevant_articles = filter_articles_sequential(new_articles)
    
    article_ids = []
    if relevant_articles:
        # Phase 3: Analyze relevant articles
        log_step(3, "Analyzing New Relevant Articles with LLM")
        analyzed_data_list = analyze_articles_sequential(relevant_articles)
        
        if analyzed_data_list:
            # Phase 4: Store results in the database
            log_step(4, "Storing New Data in Database")
            article_ids = store_analyzed_data(analyzed_data_list)
        else:
            log_warn("No new articles were successfully analyzed.")
    else:
        log_info("No new relevant articles to process.")
    
    # Phase 5: Generate the weekly report
    log_step(5, "Generating Weekly Report")
    generate_weekly_report()
    log_success("Pipeline finished successfully!")
    
    # Optional: Generate KQL Queries
    if ENABLE_KQL_GENERATION and article_ids:
        # Auto-generate if --kql flag is present, otherwise prompt
        if auto_kql or prompt_kql_generation():
            generate_kql_for_articles(article_ids)
        else:
            log_info("KQL generation skipped by user.")
    elif not article_ids:
        log_info("No new articles to generate KQL queries for.")


if __name__ == "__main__":
    DEBUG_MODE = "-debug" in sys.argv
    
    # Check for single article processing mode
    source_url = None
    if "-s" in sys.argv:
        try:
            s_index = sys.argv.index("-s")
            if s_index + 1 < len(sys.argv):
                source_url = sys.argv[s_index + 1]
        except (ValueError, IndexError):
            log_error("Invalid -s parameter. Please provide a URL.")
            sys.exit(1)
    elif "--source" in sys.argv:
        try:
            source_index = sys.argv.index("--source")
            if source_index + 1 < len(sys.argv):
                source_url = sys.argv[source_index + 1]
        except (ValueError, IndexError):
            log_error("Invalid --source parameter. Please provide a URL.")
            sys.exit(1)
    
    # If source URL is provided, process single article
    if source_url:
        use_kql = "--kql" in sys.argv or "--auto-kql" in sys.argv
        process_single_article(source_url, use_kql)
    else:
        # Normal pipeline mode
        main_pipeline()
