# main.py
import sys
import datetime
import sqlite3
from src.utils.db_utils import initialize_database, get_existing_urls, store_analyzed_data, store_iocs, store_kql_queries
from src.core.fetcher import fetch_and_scrape_articles_sequential, fetch_single_article
from src.core.filtering import filter_articles_sequential
from src.core.analysis import analyze_articles_sequential
from src.core.report import generate_weekly_report, get_last_full_week_dates
from src.utils.logging_utils import log_step, log_warn, log_info, log_success, log_error, BColors
from src.config import ENABLE_KQL_GENERATION, KQL_EXPORT_DIR, KQL_EXPORT_ENABLED, DATABASE_PATH, FETCH_DAYS_BACK
from src.core.kql_generator_llm import LLMKQLGenerator
from src.core.kql_generator import save_queries_to_file, IOCExtractor as RegexIOCExtractor


def get_rolling_date_range(days_back=14):
    """Get rolling date range for fetching articles (NOT for reports)"""
    end_date = datetime.date.today()
    start_date = end_date - datetime.timedelta(days=days_back)
    return start_date, end_date

def generate_kql_for_articles(article_ids):
    """Generate KQL queries for analyzed articles using LLM"""
    from src.utils.logging_utils import BColors
    
    log_step("KQL", "Generating KQL Threat Hunting Queries (LLM-Enhanced)")
    llm_generator = LLMKQLGenerator()
    
    total_iocs = 0
    total_queries = 0
    all_queries = []
    
    for article_id, article_data in article_ids:
        # Extract IOCs and generate queries using LLM
        iocs, queries = llm_generator.generate_all(article_data)
        ioc_count = sum(len(iocs.get(key, [])) for key in iocs)
        
        if ioc_count > 0:
            # Store IOCs
            stored_iocs = store_iocs(article_id, iocs)
            total_iocs += stored_iocs
        
        # Store and track queries (already generated by LLM)
        if queries:
            stored_queries = store_kql_queries(article_id, queries)
            total_queries += stored_queries
            all_queries.extend(queries)
            log_info(f"Generated {len(queries)} LLM queries for '{article_data['title']}'")
    
    # Export queries to files if enabled
    if KQL_EXPORT_ENABLED and all_queries:
        import os
        os.makedirs(KQL_EXPORT_DIR, exist_ok=True)
        save_queries_to_file(all_queries, KQL_EXPORT_DIR)
        log_success(f"Exported {len(all_queries)} queries to '{KQL_EXPORT_DIR}/' directory")
    
    log_success(f"KQL Generation complete: {total_iocs} IOCs, {total_queries} queries stored")
    return all_queries


def prompt_kql_generation():
    """Prompt user to generate KQL queries"""
    from logging_utils import BColors
    
    print(f"\n{BColors.HEADER}{'='*70}{BColors.ENDC}")
    print(f"{BColors.OKCYAN}ðŸ“Š KQL Threat Hunting Query Generator{BColors.ENDC}")
    print(f"{BColors.HEADER}{'='*70}{BColors.ENDC}")
    print(f"\n{BColors.OKBLUE}Would you like to generate KQL threat hunting queries?{BColors.ENDC}")
    print(f"{BColors.WARNING}This will:{BColors.ENDC}")
    print(f"  â€¢ Extract IOCs (IPs, domains, hashes, CVEs, URLs) from analyzed articles")
    print(f"  â€¢ Generate KQL queries for Microsoft Defender/Sentinel")
    print(f"  â€¢ Store queries in database and export to .kql files")
    print(f"\n{BColors.OKCYAN}Generate KQL queries? (y/n): {BColors.ENDC}", end='')
    
    try:
        response = input().strip().lower()
        return response in ['y', 'yes']
    except (KeyboardInterrupt, EOFError):
        print(f"\n{BColors.WARNING}Skipping KQL generation.{BColors.ENDC}")
        return False


def process_single_article(url, use_kql=False):
    """Process a single article from URL - fetch, analyze, and optionally generate KQL"""
    print(f"\n{BColors.HEADER}{'='*70}{BColors.ENDC}")
    print(f"{BColors.HEADER}ðŸ” Single Article Processing Mode{BColors.ENDC}")
    print(f"{BColors.HEADER}{'='*70}{BColors.ENDC}\n")
    
    # Step 1: Fetch article
    log_step(1, "Fetching Article")
    article = fetch_single_article(url)
    
    if not article:
        log_error("Failed to fetch article. Exiting.")
        return
    
    # Step 2: Filter for relevance
    log_step(2, "Checking Cybersecurity Relevance")
    relevant_articles = filter_articles_sequential([article])
    
    if not relevant_articles:
        log_warn("Article is not relevant to cybersecurity. Exiting.")
        return
    
    log_success(f"Article is relevant: {article['title']}")
    
    # Step 3: Analyze with LLM
    log_step(3, "Analyzing Article with LLM")
    analyzed_articles = analyze_articles_sequential(relevant_articles)
    
    if not analyzed_articles:
        log_error("Failed to analyze article. Exiting.")
        return
    
    analyzed_article = analyzed_articles[0]
    
    # Display analysis results
    print(f"\n{BColors.OKGREEN}{'='*70}{BColors.ENDC}")
    print(f"{BColors.OKGREEN}ðŸ“Š Analysis Results{BColors.ENDC}")
    print(f"{BColors.OKGREEN}{'='*70}{BColors.ENDC}")
    print(f"{BColors.OKCYAN}Title:{BColors.ENDC} {analyzed_article['title']}")
    print(f"{BColors.OKCYAN}Category:{BColors.ENDC} {analyzed_article.get('category', 'N/A')}")
    print(f"{BColors.OKCYAN}Threat Risk:{BColors.ENDC} {analyzed_article.get('threat_risk', 'N/A')}")
    print(f"{BColors.OKCYAN}Summary:{BColors.ENDC}")
    print(f"  {analyzed_article.get('summary', 'N/A')[:300]}...")
    print(f"{BColors.OKGREEN}{'='*70}{BColors.ENDC}\n")
    
    # Step 4: Generate KQL if requested
    if use_kql:
        log_step(4, "Generating KQL Queries (LLM + Regex)")
        
        # LLM-based extraction and query generation
        print(f"\n{BColors.OKCYAN}ðŸ¤– LLM-Based IOC Extraction & KQL Generation{BColors.ENDC}")
        print(f"{BColors.OKCYAN}{'='*70}{BColors.ENDC}")
        
        llm_generator = LLMKQLGenerator()
        llm_iocs, llm_queries = llm_generator.generate_all(analyzed_article)
        
        # Display LLM IOCs
        total_llm_iocs = sum(len(llm_iocs.get(key, [])) for key in llm_iocs)
        print(f"\n{BColors.OKGREEN}âœ… LLM Extracted {total_llm_iocs} IOCs:{BColors.ENDC}")
        
        for ioc_type, ioc_list in llm_iocs.items():
            if ioc_list:
                print(f"\n  {BColors.OKCYAN}{ioc_type.upper()}:{BColors.ENDC} ({len(ioc_list)})")
                for ioc in ioc_list[:5]:  # Show first 5
                    if isinstance(ioc, dict):
                        context = ioc.get('context', 'unknown')
                        confidence = ioc.get('confidence', 'unknown')
                        value = ioc.get('value', str(ioc))
                        desc = ioc.get('description', '')[:50]
                        print(f"    â€¢ {value}")
                        print(f"      Context: {context} | Confidence: {confidence}")
                        if desc:
                            print(f"      Description: {desc}...")
                    else:
                        print(f"    â€¢ {ioc}")
                if len(ioc_list) > 5:
                    print(f"    ... and {len(ioc_list) - 5} more")
        
        # Display LLM Queries
        print(f"\n{BColors.OKGREEN}âœ… LLM Generated {len(llm_queries)} KQL Queries:{BColors.ENDC}")
        for i, query in enumerate(llm_queries, 1):
            print(f"\n  {BColors.OKCYAN}Query {i}: {query.get('name', 'Unnamed')}{BColors.ENDC}")
            print(f"    Type: {query.get('type', 'N/A')}")
            print(f"    Platform: {query.get('platform', 'N/A')}")
            print(f"    Description: {query.get('description', 'N/A')}")
            print(f"    Tables: {', '.join(query.get('tables', []))}")
        
        # Regex-based extraction for comparison
        print(f"\n{BColors.OKCYAN}âš¡ Regex-Based IOC Extraction (for comparison){BColors.ENDC}")
        print(f"{BColors.OKCYAN}{'='*70}{BColors.ENDC}")
        
        regex_extractor = RegexIOCExtractor()
        regex_iocs = regex_extractor.extract_all(analyzed_article.get('content', ''))
        
        total_regex_iocs = sum(len(regex_iocs.get(key, [])) for key in regex_iocs)
        print(f"\n{BColors.OKGREEN}âœ… Regex Extracted {total_regex_iocs} IOCs:{BColors.ENDC}")
        
        for ioc_type, ioc_list in regex_iocs.items():
            if ioc_list:
                print(f"  {ioc_type}: {len(ioc_list)}")
                for ioc in ioc_list[:3]:  # Show first 3
                    value = ioc.get('value', str(ioc)) if isinstance(ioc, dict) else ioc
                    print(f"    â€¢ {value}")
                if len(ioc_list) > 3:
                    print(f"    ... and {len(ioc_list) - 3} more")
        
        # Export queries
        if llm_queries:
            import os
            os.makedirs(KQL_EXPORT_DIR, exist_ok=True)
            save_queries_to_file(llm_queries, KQL_EXPORT_DIR)
            log_success(f"Exported {len(llm_queries)} queries to '{KQL_EXPORT_DIR}/' directory")
        
        # Summary comparison
        print(f"\n{BColors.HEADER}{'='*70}{BColors.ENDC}")
        print(f"{BColors.HEADER}ðŸ“Š LLM vs Regex Comparison{BColors.ENDC}")
        print(f"{BColors.HEADER}{'='*70}{BColors.ENDC}")
        print(f"{BColors.OKCYAN}LLM:{BColors.ENDC} {total_llm_iocs} IOCs with context | {len(llm_queries)} queries")
        print(f"{BColors.OKCYAN}Regex:{BColors.ENDC} {total_regex_iocs} IOCs (no context) | Template-based")
        print(f"{BColors.OKGREEN}{'='*70}{BColors.ENDC}\n")
    
    log_success("Single article processing complete!")


def main_pipeline():
    # Parse the -n parameter if provided
    article_limit = None
    auto_kql = False  # Check for --auto-kql flag
    days_back = FETCH_DAYS_BACK  # Default from config
    
    if "-n" in sys.argv:
        try:
            n_index = sys.argv.index("-n")
            if n_index + 1 < len(sys.argv):
                article_limit = int(sys.argv[n_index + 1])
        except (ValueError, IndexError):
            log_warn("Invalid -n parameter. Processing all articles.")
    
    if "-t" in sys.argv:
        try:
            t_index = sys.argv.index("-t")
            if t_index + 1 < len(sys.argv):
                days_back = int(sys.argv[t_index + 1])
                log_info(f"Using custom time window: {days_back} days")
        except (ValueError, IndexError):
            log_warn(f"Invalid -t parameter. Using default {FETCH_DAYS_BACK} days.")
    
    if "--auto-kql" in sys.argv or "--kql" in sys.argv:
        auto_kql = True

    initialize_database()
    existing_urls = get_existing_urls()
    
    # Use rolling window for FETCHING (not for reports)
    fetch_start_date, fetch_end_date = get_rolling_date_range(days_back=days_back)
    
    log_info(f"Fetch window: {fetch_start_date} to {fetch_end_date} ({days_back} days)")
    
    # Phase 1: Fetch and Scrape all new articles using 2-week rolling window
    log_step(1, "Fetching and Scraping New Articles")
    new_articles = fetch_and_scrape_articles_sequential(existing_urls, fetch_start_date, fetch_end_date)
    
    # Apply article limit if specified
    if article_limit and len(new_articles) > article_limit:
        new_articles = new_articles[:article_limit]
        log_info(f"Limited to {article_limit} articles as requested.")
    
    # Phase 2: Filter for relevant articles
    log_step(2, "Filtering New Articles for Cybersecurity Relevance")
    relevant_articles = filter_articles_sequential(new_articles)
    
    article_ids = []
    if relevant_articles:
        # Phase 3: Analyze relevant articles
        log_step(3, "Analyzing New Relevant Articles with LLM")
        analyzed_data_list = analyze_articles_sequential(relevant_articles)
        
        if analyzed_data_list:
            # Phase 4: Store results in the database
            log_step(4, "Storing New Data in Database")
            article_ids = store_analyzed_data(analyzed_data_list)
        else:
            log_warn("No new articles were successfully analyzed.")
    else:
        log_info("No new relevant articles to process.")
    
    # Phase 5: Generate the weekly report
    log_step(5, "Generating Weekly Report")
    generate_weekly_report()
    log_success("Pipeline finished successfully!")
    
    # Optional: Generate KQL Queries
    if ENABLE_KQL_GENERATION and article_ids:
        # Auto-generate if --kql flag is present, otherwise prompt
        if auto_kql or prompt_kql_generation():
            generate_kql_for_articles(article_ids)
        else:
            log_info("KQL generation skipped by user.")
    elif not article_ids:
        log_info("No new articles to generate KQL queries for.")


# ============================================================================
# ENHANCED CLI COMMANDS
# ============================================================================

def cmd_list_articles(limit=20, filter_risk=None, filter_category=None):
    """List articles from database with optional filters"""
    print(f"\n{BColors.BOLD}{'='*70}{BColors.ENDC}")
    print(f"{BColors.BOLD}ðŸ“° Articles Database{BColors.ENDC}")
    print(f"{BColors.BOLD}{'='*70}{BColors.ENDC}\n")
    
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    
    # Build query with filters
    query = "SELECT id, title, category, threat_risk, published_date FROM articles"
    where_clauses = []
    params = []
    
    if filter_risk:
        where_clauses.append("threat_risk = ?")
        params.append(filter_risk.upper())
    
    if filter_category:
        where_clauses.append("category = ?")
        params.append(filter_category)
    
    if where_clauses:
        query += " WHERE " + " AND ".join(where_clauses)
    
    query += " ORDER BY published_date DESC LIMIT ?"
    params.append(limit)
    
    cursor.execute(query, params)
    articles = cursor.fetchall()
    
    if not articles:
        print(f"{BColors.WARNING}No articles found.{BColors.ENDC}\n")
        conn.close()
        return
    
    # Display articles
    for i, (article_id, title, category, risk, date) in enumerate(articles, 1):
        risk_color = {
            'HIGH': BColors.FAIL,
            'MEDIUM': BColors.WARNING,
            'LOW': BColors.OKGREEN,
            'INFORMATIONAL': BColors.OKBLUE
        }.get(risk, BColors.ENDC)
        
        print(f"{BColors.BOLD}{i}. [{article_id}]{BColors.ENDC} {title[:70]}...")
        print(f"   Category: {BColors.OKCYAN}{category}{BColors.ENDC} | Risk: {risk_color}{risk}{BColors.ENDC} | Date: {date}")
        print()
    
    # Show totals
    cursor.execute("SELECT COUNT(*) FROM articles")
    total = cursor.fetchone()[0]
    print(f"{BColors.BOLD}{'â”€'*70}{BColors.ENDC}")
    print(f"Showing {len(articles)} of {total} total articles\n")
    
    conn.close()


def cmd_search_articles(keyword, limit=10):
    """Search articles by keyword in title, content, or summary"""
    print(f"\n{BColors.BOLD}{'='*70}{BColors.ENDC}")
    print(f"{BColors.BOLD}ðŸ” Search Results for: '{keyword}'{BColors.ENDC}")
    print(f"{BColors.BOLD}{'='*70}{BColors.ENDC}\n")
    
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    
    query = """
        SELECT id, title, category, threat_risk, published_date, summary
        FROM articles
        WHERE title LIKE ? OR content LIKE ? OR summary LIKE ?
        ORDER BY published_date DESC
        LIMIT ?
    """
    
    search_term = f"%{keyword}%"
    cursor.execute(query, (search_term, search_term, search_term, limit))
    results = cursor.fetchall()
    
    if not results:
        print(f"{BColors.WARNING}No articles found matching '{keyword}'.{BColors.ENDC}\n")
        conn.close()
        return
    
    for i, (article_id, title, category, risk, date, summary) in enumerate(results, 1):
        risk_color = {
            'HIGH': BColors.FAIL,
            'MEDIUM': BColors.WARNING,
            'LOW': BColors.OKGREEN
        }.get(risk, BColors.ENDC)
        
        print(f"{BColors.BOLD}{i}. [{article_id}]{BColors.ENDC} {title}")
        print(f"   Risk: {risk_color}{risk}{BColors.ENDC} | Category: {category} | Date: {date}")
        if summary:
            print(f"   {summary[:150]}...")
        print()
    
    print(f"{BColors.BOLD}Found {len(results)} articles{BColors.ENDC}\n")
    conn.close()


def cmd_show_stats():
    """Display database statistics and insights"""
    print(f"\n{BColors.BOLD}{'='*70}{BColors.ENDC}")
    print(f"{BColors.BOLD}ðŸ“Š Threat Intelligence Statistics{BColors.ENDC}")
    print(f"{BColors.BOLD}{'='*70}{BColors.ENDC}\n")
    
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    
    # Total articles
    cursor.execute("SELECT COUNT(*) FROM articles")
    total_articles = cursor.fetchone()[0]
    print(f"{BColors.BOLD}Total Articles:{BColors.ENDC} {total_articles}")
    
    # Articles by risk level
    print(f"\n{BColors.BOLD}Risk Distribution:{BColors.ENDC}")
    cursor.execute("SELECT threat_risk, COUNT(*) FROM articles GROUP BY threat_risk ORDER BY COUNT(*) DESC")
    for risk, count in cursor.fetchall():
        risk_color = {
            'HIGH': BColors.FAIL,
            'MEDIUM': BColors.WARNING,
            'LOW': BColors.OKGREEN,
            'INFORMATIONAL': BColors.OKBLUE
        }.get(risk, BColors.ENDC)
        percentage = (count / total_articles * 100) if total_articles > 0 else 0
        bar = 'â–ˆ' * int(percentage / 2)
        print(f"  {risk_color}{risk:15}{BColors.ENDC}: {count:3} ({percentage:5.1f}%) {bar}")
    
    # Articles by category
    print(f"\n{BColors.BOLD}Top Categories:{BColors.ENDC}")
    cursor.execute("SELECT category, COUNT(*) FROM articles GROUP BY category ORDER BY COUNT(*) DESC LIMIT 5")
    for category, count in cursor.fetchall():
        print(f"  {BColors.OKCYAN}{category:20}{BColors.ENDC}: {count:3}")
    
    # Total IOCs
    cursor.execute("SELECT COUNT(*) FROM iocs")
    total_iocs = cursor.fetchone()[0]
    print(f"\n{BColors.BOLD}Total IOCs:{BColors.ENDC} {total_iocs}")
    
    # IOCs by type
    if total_iocs > 0:
        print(f"\n{BColors.BOLD}IOC Types:{BColors.ENDC}")
        cursor.execute("SELECT ioc_type, COUNT(*) FROM iocs GROUP BY ioc_type ORDER BY COUNT(*) DESC")
        for ioc_type, count in cursor.fetchall():
            print(f"  {ioc_type:15}: {count:3}")
    
    # Total KQL queries
    cursor.execute("SELECT COUNT(*) FROM kql_queries")
    total_queries = cursor.fetchone()[0]
    print(f"\n{BColors.BOLD}KQL Queries:{BColors.ENDC} {total_queries}")
    
    # Recent activity
    print(f"\n{BColors.BOLD}Recent Activity (Last 7 Days):{BColors.ENDC}")
    cursor.execute("""
        SELECT COUNT(*) FROM articles 
        WHERE published_date >= date('now', '-7 days')
    """)
    recent_count = cursor.fetchone()[0]
    print(f"  New articles: {recent_count}")
    
    # Top threats this week
    print(f"\n{BColors.BOLD}Top Threats (Last 7 Days):{BColors.ENDC}")
    cursor.execute("""
        SELECT title, threat_risk FROM articles 
        WHERE published_date >= date('now', '-7 days')
        AND threat_risk = 'HIGH'
        ORDER BY published_date DESC
        LIMIT 5
    """)
    threats = cursor.fetchall()
    if threats:
        for title, risk in threats:
            print(f"  {BColors.FAIL}â€¢{BColors.ENDC} {title[:65]}...")
    else:
        print(f"  {BColors.ENDC}No high-risk threats in the last 7 days")
    
    print(f"\n{BColors.BOLD}{'='*70}{BColors.ENDC}\n")
    conn.close()


def cmd_show_article(article_id):
    """Display detailed information about a specific article"""
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT id, title, url, category, threat_risk, published_date, summary, content
        FROM articles WHERE id = ?
    """, (article_id,))
    
    article = cursor.fetchone()
    
    if not article:
        print(f"\n{BColors.FAIL}Article ID {article_id} not found.{BColors.ENDC}\n")
        conn.close()
        return
    
    aid, title, url, category, risk, date, summary, content = article
    
    risk_color = {
        'HIGH': BColors.FAIL,
        'MEDIUM': BColors.WARNING,
        'LOW': BColors.OKGREEN
    }.get(risk, BColors.ENDC)
    
    print(f"\n{BColors.BOLD}{'='*70}{BColors.ENDC}")
    print(f"{BColors.BOLD}Article Details [{aid}]{BColors.ENDC}")
    print(f"{BColors.BOLD}{'='*70}{BColors.ENDC}\n")
    
    print(f"{BColors.BOLD}Title:{BColors.ENDC} {title}")
    print(f"{BColors.BOLD}URL:{BColors.ENDC} {url}")
    print(f"{BColors.BOLD}Category:{BColors.ENDC} {BColors.OKCYAN}{category}{BColors.ENDC}")
    print(f"{BColors.BOLD}Risk:{BColors.ENDC} {risk_color}{risk}{BColors.ENDC}")
    print(f"{BColors.BOLD}Date:{BColors.ENDC} {date}")
    
    if summary:
        print(f"\n{BColors.BOLD}Summary:{BColors.ENDC}")
        print(f"{summary}\n")
    
    # Show IOCs
    cursor.execute("SELECT ioc_type, COUNT(*) FROM iocs WHERE article_id = ? GROUP BY ioc_type", (aid,))
    iocs = cursor.fetchall()
    if iocs:
        print(f"{BColors.BOLD}IOCs Found:{BColors.ENDC}")
        for ioc_type, count in iocs:
            print(f"  {ioc_type}: {count}")
        print()
    
    # Show KQL queries
    cursor.execute("SELECT COUNT(*) FROM kql_queries WHERE article_id = ?", (aid,))
    query_count = cursor.fetchone()[0]
    if query_count > 0:
        print(f"{BColors.BOLD}KQL Queries:{BColors.ENDC} {query_count} generated")
        print()
    
    print(f"{BColors.BOLD}{'='*70}{BColors.ENDC}\n")
    conn.close()


def cmd_export_iocs(output_file="iocs_export.csv", filter_type=None):
    """Export IOCs to CSV file"""
    import csv
    
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    
    query = """
        SELECT i.ioc_type, i.ioc_value, i.context, a.title, a.published_date
        FROM iocs i
        JOIN articles a ON i.article_id = a.id
    """
    
    if filter_type:
        query += " WHERE i.ioc_type = ?"
        cursor.execute(query, (filter_type,))
    else:
        cursor.execute(query)
    
    iocs = cursor.fetchall()
    
    if not iocs:
        print(f"\n{BColors.WARNING}No IOCs found to export.{BColors.ENDC}\n")
        conn.close()
        return
    
    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Type', 'Value', 'Context', 'Article', 'Date'])
        writer.writerows(iocs)
    
    print(f"\n{BColors.OKGREEN}âœ“{BColors.ENDC} Exported {len(iocs)} IOCs to '{output_file}'\n")
    conn.close()


def cmd_export_articles(output_file="articles_export.csv", filter_risk=None, filter_category=None):
    """Export articles to CSV file"""
    import csv
    
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    
    # Build query with filters
    query = """
        SELECT id, title, url, published_date, threat_risk, category, summary
        FROM articles
    """
    where_clauses = []
    params = []
    
    if filter_risk:
        where_clauses.append("threat_risk = ?")
        params.append(filter_risk.upper())
    
    if filter_category:
        where_clauses.append("category = ?")
        params.append(filter_category)
    
    if where_clauses:
        query += " WHERE " + " AND ".join(where_clauses)
    
    query += " ORDER BY published_date DESC"
    
    cursor.execute(query, params)
    articles = cursor.fetchall()
    
    if not articles:
        print(f"\n{BColors.WARNING}No articles found to export.{BColors.ENDC}\n")
        conn.close()
        return
    
    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['ID', 'Title', 'URL', 'Published Date', 'Risk Level', 'Category', 'Summary'])
        writer.writerows(articles)
    
    print(f"\n{BColors.OKGREEN}âœ“{BColors.ENDC} Exported {len(articles)} articles to '{output_file}'\n")
    conn.close()


def cmd_analyze_unanalyzed():
    """Analyze articles in database that haven't been analyzed yet"""
    print(f"\n{BColors.BOLD}{'='*70}{BColors.ENDC}")
    print(f"{BColors.BOLD}ðŸ”¬ Analyze Unanalyzed Articles Mode{BColors.ENDC}")
    print(f"{BColors.BOLD}{'='*70}{BColors.ENDC}\n")
    
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    
    # Check for article limit
    article_limit = None
    if "-n" in sys.argv:
        try:
            n_index = sys.argv.index("-n")
            if n_index + 1 < len(sys.argv):
                article_limit = int(sys.argv[n_index + 1])
                log_info(f"Processing limit: {article_limit} articles")
        except (ValueError, IndexError):
            log_warn("Invalid -n parameter. Processing all unanalyzed articles.")
    
    # Query for unanalyzed articles
    log_step(1, "Finding Unanalyzed Articles")
    
    query = """
        SELECT id, title, url, published_date, content
        FROM articles
        WHERE threat_risk = 'UNANALYZED' OR category = 'Pending Analysis'
        ORDER BY published_date DESC
    """
    
    if article_limit:
        query += f" LIMIT {article_limit}"
    
    cursor.execute(query)
    unanalyzed_rows = cursor.fetchall()
    conn.close()
    
    if not unanalyzed_rows:
        log_info("No unanalyzed articles found in database.")
        print(f"\n{BColors.OKGREEN}âœ“{BColors.ENDC} All articles are already analyzed!")
        print(f"{BColors.OKCYAN}Tip:{BColors.ENDC} Use 'python main.py --fetch' to fetch new articles\n")
        return
    
    log_success(f"Found {len(unanalyzed_rows)} unanalyzed articles")
    
    # Convert DB rows to article format
    articles_to_analyze = []
    for row in unanalyzed_rows:
        article_id, title, url, published_date, content = row
        articles_to_analyze.append({
            'id': article_id,
            'title': title,
            'url': url,
            'published_date': published_date,
            'content': content
        })
    
    # Step 2: Filter for relevance
    log_step(2, "Filtering Articles for Cybersecurity Relevance")
    relevant_articles = filter_articles_sequential(articles_to_analyze)
    
    if not relevant_articles:
        log_warn("None of the unanalyzed articles are relevant to cybersecurity.")
        # Mark them as not relevant in DB
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        for article in articles_to_analyze:
            cursor.execute("""
                UPDATE articles
                SET threat_risk = 'NOT_RELEVANT',
                    category = 'Not Cybersecurity Related',
                    summary = 'Filtered out - not relevant to cybersecurity'
                WHERE id = ?
            """, (article['id'],))
        conn.commit()
        conn.close()
        log_info("Marked articles as not relevant in database.")
        return
    
    log_success(f"{len(relevant_articles)} articles are relevant for analysis")
    
    # Step 3: Analyze with LLM
    log_step(3, "Analyzing Articles with LLM")
    analyzed_articles = analyze_articles_sequential(relevant_articles)
    
    if not analyzed_articles:
        log_error("Failed to analyze articles.")
        return
    
    # Step 4: Update database with analysis results
    log_step(4, "Updating Database with Analysis Results")
    
    import json
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    updated_count = 0
    article_ids = []
    
    for article in analyzed_articles:
        try:
            recommendations_json = json.dumps(article.get('recommendations', []))
            cursor.execute("""
                UPDATE articles
                SET summary = ?,
                    threat_risk = ?,
                    category = ?,
                    recommendations = ?
                WHERE id = ?
            """, (
                article.get('summary', 'N/A'),
                article.get('threat_risk', 'UNKNOWN'),
                article.get('category', 'Unknown'),
                recommendations_json,
                article['id']
            ))
            updated_count += 1
            article_ids.append((article['id'], article))
        except sqlite3.Error as e:
            log_error(f"Failed to update article ID {article['id']}: {e}")
    
    conn.commit()
    conn.close()
    
    log_success(f"Updated {updated_count} articles in database")
    
    # Display summary
    print(f"\n{BColors.BOLD}{'='*70}{BColors.ENDC}")
    print(f"{BColors.BOLD}ðŸ“Š Analysis Summary{BColors.ENDC}")
    print(f"{BColors.BOLD}{'='*70}{BColors.ENDC}")
    
    risk_counts = {}
    for article in analyzed_articles:
        risk = article.get('threat_risk', 'UNKNOWN')
        risk_counts[risk] = risk_counts.get(risk, 0) + 1
    
    for risk, count in sorted(risk_counts.items()):
        risk_color = {
            'HIGH': BColors.FAIL,
            'MEDIUM': BColors.WARNING,
            'LOW': BColors.OKGREEN,
            'INFORMATIONAL': BColors.OKBLUE
        }.get(risk, BColors.ENDC)
        print(f"  {risk_color}{risk:15}{BColors.ENDC}: {count}")
    
    print(f"{BColors.BOLD}{'='*70}{BColors.ENDC}\n")
    
    # Optional: Generate KQL queries
    auto_kql = "--kql" in sys.argv or "--auto-kql" in sys.argv
    
    if ENABLE_KQL_GENERATION and article_ids:
        if auto_kql or prompt_kql_generation():
            generate_kql_for_articles(article_ids)
        else:
            log_info("KQL generation skipped by user.")
    
    log_success("Analysis complete!")


def cmd_fetch_only():
    """Fetch articles only without filtering or analysis"""
    print(f"\n{BColors.BOLD}{'='*70}{BColors.ENDC}")
    print(f"{BColors.BOLD}ðŸ“¥ Fetch Only Mode{BColors.ENDC}")
    print(f"{BColors.BOLD}{'='*70}{BColors.ENDC}\n")
    
    # Check for -t parameter
    days_back = FETCH_DAYS_BACK
    if "-t" in sys.argv:
        try:
            t_index = sys.argv.index("-t")
            if t_index + 1 < len(sys.argv):
                days_back = int(sys.argv[t_index + 1])
                log_info(f"Using custom time window: {days_back} days")
        except (ValueError, IndexError):
            log_warn(f"Invalid -t parameter. Using default {FETCH_DAYS_BACK} days.")
    
    initialize_database()
    existing_urls = get_existing_urls()
    
    # Use rolling window for fetching
    fetch_start_date, fetch_end_date = get_rolling_date_range(days_back=days_back)
    
    log_info(f"Fetch window: {fetch_start_date} to {fetch_end_date} ({days_back} days)")
    
    # Fetch articles
    log_step(1, "Fetching Articles from RSS Feeds")
    new_articles = fetch_and_scrape_articles_sequential(existing_urls, fetch_start_date, fetch_end_date)
    
    if not new_articles:
        log_warn("No new articles found.")
        return
    
    # Store raw articles in database without analysis
    log_step(2, "Storing Raw Articles in Database")
    
    stored_count = 0
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    
    for article in new_articles:
        try:
            cursor.execute("""
                INSERT INTO articles (title, url, published_date, content, summary, threat_risk, category, recommendations)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                article['title'],
                article['url'],
                article.get('published_date', datetime.date.today().isoformat()),
                article.get('content', ''),
                'Not analyzed - fetched only',
                'UNANALYZED',
                'Pending Analysis',
                '[]'
            ))
            stored_count += 1
        except sqlite3.IntegrityError:
            # Skip duplicates
            continue
        except Exception as e:
            log_error(f"Failed to store article '{article['title']}': {e}")
    
    conn.commit()
    conn.close()
    
    log_success(f"Successfully stored {stored_count} raw articles in database")
    log_info(f"Use 'python main.py --analyze' to analyze these articles later")
    
    print(f"\n{BColors.BOLD}{'='*70}{BColors.ENDC}")
    print(f"{BColors.OKGREEN}âœ“{BColors.ENDC} Fetch complete! Articles are stored but not analyzed.")
    print(f"{BColors.OKCYAN}Tip:{BColors.ENDC} Run 'python main.py --analyze' to analyze these articles")
    print(f"{BColors.BOLD}{'='*70}{BColors.ENDC}\n")


def cmd_show_help():
    """Display help message with all available commands"""
    help_text = f"""
{BColors.BOLD}{'='*80}{BColors.ENDC}
{BColors.BOLD}ðŸ”§ Threat Intelligence Pipeline - Complete Command Reference{BColors.ENDC}
{BColors.BOLD}{'='*80}{BColors.ENDC}

{BColors.HEADER}PIPELINE MODES:{BColors.ENDC}
  {BColors.OKGREEN}python main.py{BColors.ENDC}
      Run full pipeline: Fetch articles (2-week window), analyze, generate report

  {BColors.OKGREEN}python main.py --fetch{BColors.ENDC}
      Fetch-only mode: Download articles from RSS feeds and store in database
      (no filtering, no analysis, no report generation)

  {BColors.OKGREEN}python main.py --analyze{BColors.ENDC}
      Analyze-only mode: Process articles in database that are not yet analyzed
      (useful after using --fetch, or when analysis previously failed)

  {BColors.OKGREEN}python main.py -n <N>{BColors.ENDC}
      Limit processing to N articles (useful for testing)
      Example: python main.py -n 20
      Works with: main pipeline, --fetch, --analyze

  {BColors.OKGREEN}python main.py -t <DAYS>{BColors.ENDC}
      Fetch articles from last N days (default: 14)
      Example: python main.py -t 30 (fetch from last 30 days)
      Works with: main pipeline, --fetch

  {BColors.OKGREEN}python main.py --kql{BColors.ENDC} or {BColors.OKGREEN}--auto-kql{BColors.ENDC}
      Run pipeline and automatically generate KQL queries
      Works with: main pipeline, --analyze, single article mode

  {BColors.OKGREEN}python main.py -s <URL>{BColors.ENDC} or {BColors.OKGREEN}--source <URL>{BColors.ENDC}
      Process a single article from URL (fetch, analyze, store)
      Example: python main.py -s https://example.com/article --kql

  {BColors.OKGREEN}python main.py -debug{BColors.ENDC}
      Enable debug mode for troubleshooting

{BColors.HEADER}DATABASE QUERY COMMANDS:{BColors.ENDC}
  {BColors.OKCYAN}--list{BColors.ENDC}
      List articles from database (default: 20 most recent)
      Example: python main.py --list
      
  {BColors.OKCYAN}--list --limit <N>{BColors.ENDC}
      List N most recent articles
      Example: python main.py --list --limit 50

  {BColors.OKCYAN}--list --risk <LEVEL>{BColors.ENDC}
      Filter articles by risk level: HIGH, MEDIUM, LOW, INFORMATIONAL
      Example: python main.py --list --risk HIGH

  {BColors.OKCYAN}--list --category <NAME>{BColors.ENDC}
      Filter articles by category (e.g., Malware, Ransomware, APT)
      Example: python main.py --list --category "Ransomware"

  {BColors.OKCYAN}--search <keyword>{BColors.ENDC}
      Search articles by keyword in title, content, or summary
      Example: python main.py --search "WSUS vulnerability"

  {BColors.OKCYAN}--search <keyword> --limit <N>{BColors.ENDC}
      Search with custom result limit
      Example: python main.py --search "APT36" --limit 10

  {BColors.OKCYAN}--show <ID>{BColors.ENDC}
      Display detailed information for specific article ID
      Example: python main.py --show 42

  {BColors.OKCYAN}--stats{BColors.ENDC}
      Show database statistics and threat insights

{BColors.HEADER}EXPORT COMMANDS:{BColors.ENDC}
  {BColors.WARNING}--export-articles{BColors.ENDC}
      Export all articles to CSV file (default: articles_export.csv)
      Example: python main.py --export-articles

  {BColors.WARNING}--export-articles --output <file>{BColors.ENDC}
      Export articles to custom filename
      Example: python main.py --export-articles --output my_articles.csv

  {BColors.WARNING}--export-articles --risk <LEVEL>{BColors.ENDC}
      Export only articles with specific risk level
      Example: python main.py --export-articles --risk HIGH

  {BColors.WARNING}--export-articles --category <NAME>{BColors.ENDC}
      Export only articles from specific category
      Example: python main.py --export-articles --category Ransomware

  {BColors.WARNING}--export-iocs{BColors.ENDC}
      Export all IOCs to CSV file (default: iocs_export.csv)
      Example: python main.py --export-iocs

  {BColors.WARNING}--export-iocs --output <file>{BColors.ENDC}
      Export IOCs to custom filename
      Example: python main.py --export-iocs --output my_iocs.csv

  {BColors.WARNING}--export-iocs --type <IOC_TYPE>{BColors.ENDC}
      Export only specific IOC type: domains, ips, hashes, cves, urls
      Example: python main.py --export-iocs --type domains

{BColors.HEADER}CONFIGURATION PARAMETERS:{BColors.ENDC}
  {BColors.OKBLUE}-n <N>{BColors.ENDC}
      Limit article processing to N articles
      
  {BColors.OKBLUE}-t <DAYS>{BColors.ENDC}
      Fetch articles from last N days (default: 14 from config)
      Example: -t 30 for 30-day window
      
  {BColors.OKBLUE}--limit <N>{BColors.ENDC}
      Limit search/list results to N items
      
  {BColors.OKBLUE}--risk <LEVEL>{BColors.ENDC}
      Filter by risk: HIGH, MEDIUM, LOW, INFORMATIONAL
      
  {BColors.OKBLUE}--category <NAME>{BColors.ENDC}
      Filter by category name
      
  {BColors.OKBLUE}--type <IOC_TYPE>{BColors.ENDC}
      Filter IOCs by type: domains, ips, hashes, cves, urls
      
  {BColors.OKBLUE}--output <file>{BColors.ENDC}
      Specify output filename for exports

{BColors.HEADER}HELP & INFO:{BColors.ENDC}
  {BColors.FAIL}--help{BColors.ENDC} or {BColors.FAIL}-h{BColors.ENDC}
      Display this help message

{BColors.HEADER}CONFIGURATION (in config.py):{BColors.ENDC}
  FETCH_DAYS_BACK = 14              Fetch articles from last 14 days (rolling window)
  MIN_ARTICLE_LENGTH = 100          Minimum content length in characters
  FETCH_TIMEOUT = 30                Request timeout in seconds
  SOCKET_TIMEOUT = 30               Socket timeout for RSS feeds
  ENABLE_KQL_GENERATION = True      Enable/disable KQL generation feature
  KQL_USE_LLM = True                Use LLM for IOC extraction (recommended)

{BColors.HEADER}EXAMPLE WORKFLOWS:{BColors.ENDC}
  {BColors.BOLD}1. Daily Threat Intel Gathering:{BColors.ENDC}
     python main.py --auto-kql

  {BColors.BOLD}2. Fetch Now, Analyze Later:{BColors.ENDC}
     python main.py --fetch         # Fetch articles during work hours
     python main.py --analyze --kql # Analyze them later (evenings/weekends)

  {BColors.BOLD}3. Quick Fetch (No Analysis):{BColors.ENDC}
     python main.py --fetch

  {BColors.BOLD}4. Batch Fetch & Selective Analysis:{BColors.ENDC}
     python main.py --fetch -t 30   # Fetch 30 days of articles
     python main.py --analyze -n 50 # Analyze 50 most recent unanalyzed

  {BColors.BOLD}5. Fetch 30 Days of Articles:{BColors.ENDC}
     python main.py -t 30

  {BColors.BOLD}6. Quick Test Run:{BColors.ENDC}
     python main.py -n 10

  {BColors.BOLD}7. Analyze Single Threat Article:{BColors.ENDC}
     python main.py -s https://blog.example.com/new-ransomware --kql

  {BColors.BOLD}8. Find All High-Risk Threats:{BColors.ENDC}
     python main.py --list --risk HIGH --limit 100

  {BColors.BOLD}9. Search & Export IOCs:{BColors.ENDC}
     python main.py --search "APT" --limit 20
     python main.py --export-iocs --type domains

  {BColors.BOLD}10. Database Overview:{BColors.ENDC}
     python main.py --stats

  {BColors.BOLD}11. Review Specific Threat:{BColors.ENDC}
     python main.py --show 15

{BColors.BOLD}{'='*80}{BColors.ENDC}
{BColors.OKCYAN}ðŸ“– For more information, see README.md or docs/ folder{BColors.ENDC}
{BColors.BOLD}{'='*80}{BColors.ENDC}
"""
    print(help_text)


if __name__ == "__main__":
    DEBUG_MODE = "-debug" in sys.argv
    
    # Helper function to get argument value
    def get_arg_value(arg_name, default=None):
        """Get value for an argument"""
        if arg_name in sys.argv:
            try:
                idx = sys.argv.index(arg_name)
                if idx + 1 < len(sys.argv):
                    return sys.argv[idx + 1]
            except (ValueError, IndexError):
                pass
        return default
    
    # Check for CLI query commands first (these don't run the pipeline)
    if "--help" in sys.argv or "-h" in sys.argv:
        cmd_show_help()
        sys.exit(0)
    
    elif "--fetch" in sys.argv:
        cmd_fetch_only()
        sys.exit(0)
    
    elif "--analyze" in sys.argv:
        cmd_analyze_unanalyzed()
        sys.exit(0)
    
    elif "--stats" in sys.argv:
        cmd_show_stats()
        sys.exit(0)
    
    elif "--list" in sys.argv:
        limit = int(get_arg_value("--limit", 20))
        risk_filter = get_arg_value("--risk")
        category_filter = get_arg_value("--category")
        cmd_list_articles(limit=limit, filter_risk=risk_filter, filter_category=category_filter)
        sys.exit(0)
    
    elif "--search" in sys.argv:
        keyword = get_arg_value("--search")
        if not keyword:
            log_error("Please provide a search keyword: --search <keyword>")
            sys.exit(1)
        limit = int(get_arg_value("--limit", 10))
        cmd_search_articles(keyword, limit=limit)
        sys.exit(0)
    
    elif "--show" in sys.argv:
        article_id = get_arg_value("--show")
        if not article_id:
            log_error("Please provide an article ID: --show <id>")
            sys.exit(1)
        try:
            cmd_show_article(int(article_id))
        except ValueError:
            log_error("Article ID must be a number")
            sys.exit(1)
        sys.exit(0)
    
    elif "--export-iocs" in sys.argv:
        output_file = get_arg_value("--output", "iocs_export.csv")
        filter_type = get_arg_value("--type")
        cmd_export_iocs(output_file=output_file, filter_type=filter_type)
        sys.exit(0)
    
    elif "--export-articles" in sys.argv:
        output_file = get_arg_value("--output", "articles_export.csv")
        risk_filter = get_arg_value("--risk")
        category_filter = get_arg_value("--category")
        cmd_export_articles(output_file=output_file, filter_risk=risk_filter, filter_category=category_filter)
        sys.exit(0)
    
    # Check for single article processing mode
    source_url = None
    if "-s" in sys.argv:
        try:
            s_index = sys.argv.index("-s")
            if s_index + 1 < len(sys.argv):
                source_url = sys.argv[s_index + 1]
        except (ValueError, IndexError):
            log_error("Invalid -s parameter. Please provide a URL.")
            sys.exit(1)
    elif "--source" in sys.argv:
        try:
            source_index = sys.argv.index("--source")
            if source_index + 1 < len(sys.argv):
                source_url = sys.argv[source_index + 1]
        except (ValueError, IndexError):
            log_error("Invalid --source parameter. Please provide a URL.")
            sys.exit(1)
    
    # If source URL is provided, process single article
    if source_url:
        use_kql = "--kql" in sys.argv or "--auto-kql" in sys.argv
        process_single_article(source_url, use_kql)
    else:
        # Normal pipeline mode
        main_pipeline()
